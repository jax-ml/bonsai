{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2769b54b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --quiet flax optax einops pillow transformers matplotlib tqdm scikit-learn seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d2fe87",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Copyright 2025 The Bonsai AI Authors.\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "\n",
    "import os, pickle, random\n",
    "import numpy as np\n",
    "import jax, jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "from bonsai.models.clip_jax.modeling import CLIPModel\n",
    "from bonsai.models.clip_jax.config import CLIPConfig\n",
    "from bonsai.models.clip_jax.utils.preprocess import preprocess_image, tokenize_text\n",
    "\n",
    "# Paths ‚Äî use local if available, else Kaggle path\n",
    "\n",
    "ADE_PATH = \"datasets/ADEChallengeData2016\"\n",
    "if not os.path.exists(ADE_PATH):\n",
    "    ADE_PATH = \"/kaggle/input/ade20k-dataset/ADEChallengeData2016\"\n",
    "\n",
    "CKPT_PATH = \"clip_jax/ckpts/clip_ade20k_epoch1.pkl\"\n",
    "\n",
    "cfg = CLIPConfig()\n",
    "model = CLIPModel(cfg)\n",
    "\n",
    "# Load model parameters\n",
    "if not os.path.exists(CKPT_PATH):\n",
    "    raise FileNotFoundError(f\"Checkpoint not found at {CKPT_PATH}\")\n",
    "\n",
    "with open(CKPT_PATH, \"rb\") as f:\n",
    "    params = pickle.load(f)\n",
    "\n",
    "print(\"‚úÖ Loaded CLIP-JAX checkpoint successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b70e3c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load ADE20K scene label mapping\n",
    "scene_labels = {}\n",
    "scene_txt = os.path.join(ADE_PATH, \"sceneCategories.txt\")\n",
    "\n",
    "if os.path.exists(scene_txt):\n",
    "    with open(scene_txt, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                scene_labels[parts[0]] = parts[1]\n",
    "else:\n",
    "    raise FileNotFoundError(\"sceneCategories.txt not found in ADE20K dataset.\")\n",
    "\n",
    "all_classes = sorted(list(set(scene_labels.values())))\n",
    "print(f\"üéØ Found {len(all_classes)} unique ADE20K scene classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d7e2a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Randomly choose 5 scene classes for visualization\n",
    "\n",
    "selected_classes = random.sample(all_classes, 5)\n",
    "print(f\"üé® Selected classes: {selected_classes}\")\n",
    "\n",
    "val_dir = os.path.join(ADE_PATH, \"images\", \"validation\")\n",
    "img_paths, txt_labels = [], []\n",
    "\n",
    "# Collect up to 10 validation images per selected class\n",
    "\n",
    "for cls in selected_classes:\n",
    "    matched = [\n",
    "        os.path.join(val_dir, f\"{k}.jpg\")\n",
    "        for k, v in scene_labels.items()\n",
    "        if v == cls and os.path.exists(os.path.join(val_dir, f\"{k}.jpg\"))\n",
    "    ]\n",
    "    img_paths.extend(matched[:10])\n",
    "    txt_labels.extend([cls] * min(10, len(matched)))\n",
    "\n",
    "print(f\"üì∏ Collected {len(img_paths)} images from {len(selected_classes)} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3362e0c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compute image embeddings\n",
    "img_thumbs, img_embs = [], []\n",
    "\n",
    "for path, lbl in zip(img_paths, txt_labels):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\").resize((cfg.image_size, cfg.image_size))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipping image {path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    img_thumbs.append(np.array(img))\n",
    "    img_jax = preprocess_image(img, cfg.image_size)[None, ...]\n",
    "    tok = tokenize_text([f\"a photo of a {lbl}\"], cfg.text_max_len)\n",
    "\n",
    "    _, img_e, _ = model.apply(params, img_jax, tok, train=False)\n",
    "    img_embs.append(np.array(img_e[0]))\n",
    "\n",
    "img_embs = np.stack(img_embs)\n",
    "print(f\"‚úÖ Computed embeddings for {len(img_embs)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a702e9f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Compute text embeddings\n",
    "txt_prompts = [f\"a photo of a {c}\" for c in selected_classes]\n",
    "tok = tokenize_text(txt_prompts, cfg.text_max_len)\n",
    "\n",
    "_, txt_embs, _ = model.apply(\n",
    "    params,\n",
    "    jnp.zeros((len(txt_prompts), cfg.image_size, cfg.image_size, 3)),\n",
    "    tok,\n",
    "    train=False,\n",
    ")\n",
    "txt_embs = np.array(txt_embs)\n",
    "\n",
    "print(f\"‚úÖ Computed text embeddings for {len(selected_classes)} prompts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796675c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Combine and run t-SNE on embeddings\n",
    "emb = np.concatenate([img_embs, txt_embs], axis=0)\n",
    "n_samples = emb.shape[0]\n",
    "perplexity = max(5, min(30, n_samples // 3))\n",
    "\n",
    "print(f\"üß© Running t-SNE on {n_samples} samples (perplexity={perplexity})...\")\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=perplexity,\n",
    "    init=\"pca\",\n",
    "    learning_rate=\"auto\",\n",
    "    random_state=42,\n",
    "    n_iter=1500,\n",
    ")\n",
    "pts = tsne.fit_transform(emb)\n",
    "\n",
    "num_img = len(img_embs)\n",
    "img_pts, txt_pts = pts[:num_img], pts[num_img:]\n",
    "print(\"‚úÖ t-SNE embedding complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a2106",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization setup\n",
    "palette = sns.color_palette(\"tab10\", n_colors=len(selected_classes))\n",
    "cls_to_color = {cls: palette[i] for i, cls in enumerate(selected_classes)}\n",
    "\n",
    "plt.figure(figsize=(22, 18))\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor(\"white\")\n",
    "\n",
    "# Plot text anchors\n",
    "for i, cls in enumerate(selected_classes):\n",
    "    x, y = txt_pts[i]\n",
    "    ax.scatter(x, y, s=700, facecolor=cls_to_color[cls],\n",
    "               edgecolors=\"black\", linewidth=1.5, zorder=3)\n",
    "    ax.text(\n",
    "        x, y + 12,\n",
    "        cls.replace(\"_\", \" \"),\n",
    "        fontsize=20, fontweight=\"bold\",\n",
    "        color=\"black\", ha=\"center\", va=\"bottom\",\n",
    "        bbox=dict(facecolor=\"white\", alpha=0.9, edgecolor=\"none\", boxstyle=\"round,pad=0.5\"),\n",
    "        zorder=4\n",
    "    )\n",
    "\n",
    "# Plot image thumbnails\n",
    "for (x, y), thumb, cls in zip(img_pts, img_thumbs, txt_labels):\n",
    "    im = OffsetImage(thumb, zoom=1.6, resample=True)\n",
    "    ab = AnnotationBbox(im, (x, y), frameon=False, pad=0.15)\n",
    "    ax.add_artist(ab)\n",
    "    ax.plot(x, y, \"o\", color=cls_to_color[cls], markersize=10, alpha=0.4, zorder=2)\n",
    "\n",
    "plt.title(\n",
    "    \"üß© CLIP-JAX ADE20K t-SNE Visualization ‚Äî Images and Text Prompts\",\n",
    "    fontsize=26, pad=40, weight=\"bold\"\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c6314",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save cache for reuse\n",
    "\n",
    "os.makedirs(\"clip_jax/ckpts\", exist_ok=True)\n",
    "cache_path = \"clip_jax/ckpts/tsne_cache.pkl\"\n",
    "\n",
    "with open(cache_path, \"wb\") as f:\n",
    "    pickle.dump((img_pts, txt_pts, selected_classes, txt_labels, cls_to_color), f)\n",
    "\n",
    "print(f\"üíæ Saved cached t-SNE embeddings ‚Üí {cache_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc281622",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(f\"\"\"\n",
    "‚úÖ Zero-shot t-SNE visualization complete!\n",
    "üì¶ Model: CLIP-JAX (trained on ADE20K)\n",
    "üéØ Classes visualized: {', '.join(selected_classes)}\n",
    "üíæ Cache saved: clip_jax/ckpts/tsne_cache.pkl\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
