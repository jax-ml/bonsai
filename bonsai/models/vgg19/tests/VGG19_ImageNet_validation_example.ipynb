{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/eari100/bonsai/blob/vgg-19/bonsai/models/vgg19/tests/VGG19_ImageNet_validation_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggested runtime: GPU (A100/H100) or TPU v2-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ImageNet Classification with VGG-19**\n",
    "\n",
    "This notebook demonstrates how to use the VGG-19 model from the Bonsai library to perform ImageNet classification on real images. The model is pre-trained on ImageNet-1K and can classify images into 1000 different categories.\n",
    "\n",
    "*This colab demonstrates the VGG-19 implementation from the [Bonsai library](https://github.com/jax-ml/bonsai).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Set-up**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -q git+https://github.com/eari100/bonsai@vgg-19\n",
    "!pip install -q pillow matplotlib requests"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "from huggingface_hub import snapshot_download\n",
    "from jax.lib import xla_bridge\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX device: {xla_bridge.get_backend().platform}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Download Sample Images**\n",
    "\n",
    "Let's download some sample images to test our VGG-19 model. We'll use images that are commonly used for testing image classification models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create images directory\n",
    "os.makedirs(\"./images\", exist_ok=True)\n",
    "\n",
    "# Download sample images\n",
    "!wget -q -P ./images/ tench.jpg \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n01440764_tench.JPEG\"\n",
    "!wget -q -P ./images/ goldfish.jpg \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n01443537_goldfish.JPEG\"\n",
    "!wget -q -P ./images/ tiger_shark.jpg \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n01491361_tiger_shark.JPEG\"\n",
    "!wget -q -P ./images/ hammerhead.jpg \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n01494475_hammerhead.JPEG\"\n",
    "!wget -q -P ./images/ electric_ray.jpg \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n01496331_electric_ray.JPEG\"\n",
    "\n",
    "\" \".join(os.listdir(\"./images\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load ImageNet Class Names**\n",
    "\n",
    "We need to load the ImageNet class names to interpret the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_imagenet_classes():\n",
    "    \"\"\"Load ImageNet class names from a common source.\"\"\"\n",
    "    # Download ImageNet class names\n",
    "    url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise HTTPError for bad responses (4xx, 5xx)\n",
    "    classes = response.text.strip().split(\"\\n\")\n",
    "    return classes\n",
    "\n",
    "\n",
    "try:\n",
    "    imagenet_classes = load_imagenet_classes()\n",
    "    print(f\"Loaded {len(imagenet_classes)} ImageNet classes\")\n",
    "    print(\"Sample classes:\", imagenet_classes[:5])\n",
    "except requests.exceptions.RequestException:\n",
    "    # Fallback to a subset of common classes\n",
    "    print(\"Could not load ImageNet classes, using fallback\")\n",
    "    imagenet_classes = [f\"class_{i}\" for i in range(1000)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load VGG-19 Model**\n",
    "\n",
    "Now let's load the pre-trained VGG-19 model from the Bonsai library."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from flax import nnx\n",
    "\n",
    "from bonsai.models.vgg19 import modeling as model_lib\n",
    "from bonsai.models.vgg19 import params\n",
    "\n",
    "\n",
    "def load_vgg19_model():\n",
    "    \"\"\"Load the pre-trained VGG-19 model.\"\"\"\n",
    "    # Download model weights\n",
    "    model_name = \"keras/vgg_19_imagenet\"\n",
    "\n",
    "    print(f\"Downloading {model_name}...\")\n",
    "    model_ckpt_path = snapshot_download(model_name)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "    # Load the model\n",
    "    config = model_lib.ModelCfg.vgg_19()\n",
    "    model = params.create_model_from_h5(model_ckpt_path, config)\n",
    "\n",
    "    print(\"VGG-19 model loaded successfully!\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = load_vgg19_model()\n",
    "graphdef, state = nnx.split(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Image Preprocessing Functions**\n",
    "\n",
    "VGG-19 expects images to be preprocessed in a specific way: resized to 224x224, normalized with ImageNet statistics, and converted to the right format."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"Preprocess image for VGG-19 inference.\"\"\"\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Resize image\n",
    "    image = ImageOps.fit(image, target_size, method=Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Convert to numpy array and normalize to [0, 1]\n",
    "    image_array = np.array(image).astype(np.float32) / 255.0\n",
    "\n",
    "    # ImageNet normalization: mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "    # Normalize\n",
    "    image_array = (image_array - mean) / std\n",
    "\n",
    "    # Add batch dimension and ensure correct shape (batch, height, width, channels)\n",
    "    image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "    return image_array, image\n",
    "\n",
    "\n",
    "def show_image_with_predictions(image, predictions, top_k=5):\n",
    "    \"\"\"Display image with top-k predictions.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Show image\n",
    "    ax1.imshow(image)\n",
    "    ax1.set_title(\"Input Image\", fontsize=14)\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    # Show predictions\n",
    "    y_pos = np.arange(len(predictions))\n",
    "    ax2.barh(y_pos, [p[1] for p in predictions])\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels([p[0] for p in predictions])\n",
    "    ax2.set_xlabel(\"Confidence Score\")\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels([p[0] for p in predictions])\n",
    "    ax2.set_xlabel(\"Confidence Score\")\n",
    "    ax2.set_title(f\"Top-{top_k} Predictions\", fontsize=14)\n",
    "    ax2.invert_yaxis()  # Top prediction at the top\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run Inference on Sample Images**\n",
    "\n",
    "Now let's test our VGG-19 model on the downloaded images!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def classify_image(graphdef, state, image_path, top_k=5):\n",
    "    \"\"\"Classify a single image and return top-k predictions.\"\"\"\n",
    "    # Preprocess image\n",
    "    input_tensor, original_image = preprocess_image(image_path)\n",
    "\n",
    "    # Convert to JAX array\n",
    "    input_tensor = jnp.array(input_tensor)\n",
    "\n",
    "    # Run inference\n",
    "    logits = model_lib.forward(graphdef, state, input_tensor)\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = jax.nn.softmax(logits[0])\n",
    "\n",
    "    # Get top-k predictions\n",
    "    top_indices = jnp.argsort(probabilities)[-top_k:][::-1]\n",
    "    top_probs = probabilities[top_indices]\n",
    "\n",
    "    # Format results\n",
    "    predictions = []\n",
    "    for idx, prob in zip(top_indices, top_probs):\n",
    "        class_name = imagenet_classes[idx] if idx < len(imagenet_classes) else f\"class_{idx}\"\n",
    "        predictions.append((class_name, float(prob)))\n",
    "\n",
    "    return predictions, original_image\n",
    "\n",
    "\n",
    "# Test on all images\n",
    "image_files = [f for f in os.listdir(\"./images\") if f.endswith((\".jpg\", \".jpeg\", \".png\", \".JPEG\"))]\n",
    "\n",
    "for image_file in image_files:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Classifying: {image_file}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    try:\n",
    "        predictions, image = classify_image(graphdef, state, f\"./images/{image_file}\")\n",
    "\n",
    "        print(\"\\nTop-5 predictions:\")\n",
    "        for i, (class_name, confidence) in enumerate(predictions, 1):\n",
    "            print(f\"{i}. {class_name}: {confidence:.4f} ({confidence * 100:.2f}%)\")\n",
    "\n",
    "        # Show visualization\n",
    "        show_image_with_predictions(image, predictions)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_file}: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Batch Processing**\n",
    "\n",
    "Let's also demonstrate batch processing for multiple images at once, which is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def batch_classify_images(graphdef, state, image_paths, top_k=5):\n",
    "    \"\"\"Classify multiple images in a single batch.\"\"\"\n",
    "    # Preprocess all images\n",
    "    input_tensors = []\n",
    "    original_images = []\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        input_tensor, original_image = preprocess_image(image_path)\n",
    "        input_tensors.append(input_tensor[0])  # Remove batch dimension\n",
    "        original_images.append(original_image)\n",
    "\n",
    "    # Stack into batch\n",
    "    batch_tensor = jnp.stack(input_tensors)\n",
    "\n",
    "    # Run batch inference\n",
    "    logits = model_lib.forward(graphdef, state, batch_tensor)\n",
    "\n",
    "    # Process results\n",
    "    all_predictions = []\n",
    "    for i in range(len(image_paths)):\n",
    "        probabilities = jax.nn.softmax(logits[i])\n",
    "        top_indices = jnp.argsort(probabilities)[-top_k:][::-1]\n",
    "        top_probs = probabilities[top_indices]\n",
    "\n",
    "        predictions = []\n",
    "        for idx, prob in zip(top_indices, top_probs):\n",
    "            class_name = imagenet_classes[idx] if idx < len(imagenet_classes) else f\"class_{idx}\"\n",
    "            predictions.append((class_name, float(prob)))\n",
    "\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "    return all_predictions, original_images\n",
    "\n",
    "\n",
    "# Test batch processing\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BATCH PROCESSING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "image_paths = [f\"./images/{f}\" for f in image_files[:3]]  # Process first 3 images\n",
    "batch_predictions, batch_images = batch_classify_images(graphdef, state, image_paths)\n",
    "\n",
    "for i, (image_path, predictions, image) in enumerate(zip(image_paths, batch_predictions, batch_images)):\n",
    "    print(f\"\\nImage {i + 1}: {os.path.basename(image_path)}\")\n",
    "    print(\"-\" * 40)\n",
    "    for j, (class_name, confidence) in enumerate(predictions, 1):\n",
    "        print(f\"{j}. {class_name}: {confidence:.4f} ({confidence * 100:.2f}%)\")\n",
    "\n",
    "    # Show individual image with predictions\n",
    "    show_image_with_predictions(image, predictions)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "This notebook demonstrates how to use the VGG-19 model from the Bonsai library to perform ImageNet classification on real images. The model successfully:\n",
    "\n",
    "1. **Loads pre-trained weights** from Keras's VGG-19 model\n",
    "2. **Preprocesses images** according to ImageNet standards\n",
    "3. **Performs inference** on individual and batched images\n",
    "4. **Provides confidence scores** for top-k predictions\n",
    "\n",
    "The implementation shows that the Bonsai VGG-19 model works correctly for real-world image classification tasks"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ResNet50 ImageNet Validation Example",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
