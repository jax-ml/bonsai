{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab873b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "import optax\n",
    "\n",
    "try:\n",
    "    from bonsai.models.gat.modeling import GAT\n",
    "    from bonsai.models.gat.params import GATConfig\n",
    "except ImportError:\n",
    "    try:\n",
    "        !pip insetall -e .\n",
    "    except:\n",
    "        !pip install -q git+https://github.com/jax-ml/bonsai@main\n",
    "\n",
    "\n",
    "# Configuration\n",
    "# Use a temporary directory for data to avoid polluting the repo\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"data\", \"cora\")\n",
    "CORA_CONTENT_URL = \"https://raw.githubusercontent.com/tkipf/pygcn/master/data/cora/cora.content\"\n",
    "CORA_CITES_URL = \"https://raw.githubusercontent.com/tkipf/pygcn/master/data/cora/cora.cites\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0aee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_cora():\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    content_path = os.path.join(DATA_DIR, \"cora.content\")\n",
    "    cites_path = os.path.join(DATA_DIR, \"cora.cites\")\n",
    "\n",
    "    if not os.path.exists(content_path):\n",
    "        print(f\"Downloading {CORA_CONTENT_URL}...\")\n",
    "        urllib.request.urlretrieve(CORA_CONTENT_URL, content_path)\n",
    "\n",
    "    if not os.path.exists(cites_path):\n",
    "        print(f\"Downloading {CORA_CITES_URL}...\")\n",
    "        urllib.request.urlretrieve(CORA_CITES_URL, cites_path)\n",
    "\n",
    "    return content_path, cites_path\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    content_path, cites_path = download_cora()\n",
    "\n",
    "    # Load content\n",
    "    # Format: paper_id, word_attributes..., label\n",
    "    content = np.genfromtxt(content_path, dtype=np.dtype(str))\n",
    "    idx = content[:, 0].astype(np.int32)\n",
    "    features = content[:, 1:-1].astype(np.float32)\n",
    "    labels_str = content[:, -1]\n",
    "\n",
    "    # Map labels to integers\n",
    "    unique_labels = sorted(set(labels_str))\n",
    "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "    labels = np.array([label_map[l] for l in labels_str], dtype=np.int32)\n",
    "\n",
    "    # Map paper IDs to 0-N\n",
    "    idx_map = {id: i for i, id in enumerate(idx)}\n",
    "\n",
    "    # Load cites\n",
    "    # Format: cited_paper_id, citing_paper_id\n",
    "    edges_unordered = np.genfromtxt(cites_path, dtype=np.int32)\n",
    "\n",
    "    edges = []\n",
    "    for edge in edges_unordered:\n",
    "        if edge[0] in idx_map and edge[1] in idx_map:\n",
    "            edges.append([idx_map[edge[0]], idx_map[edge[1]]])\n",
    "    edges = np.array(edges)\n",
    "\n",
    "    N = features.shape[0]\n",
    "    adj = np.zeros((N, N), dtype=np.float32)\n",
    "    adj[edges[:, 0], edges[:, 1]] = 1.0\n",
    "    # Symmetric adjacency matrix\n",
    "    adj = adj + adj.T\n",
    "    adj = np.clip(adj, 0, 1)\n",
    "\n",
    "    # Add self-loops\n",
    "    adj = adj + np.eye(N)\n",
    "\n",
    "    # Row-normalize features\n",
    "    features_sum = features.sum(axis=1, keepdims=True)\n",
    "    features_sum = np.where(features_sum == 0, 1, features_sum)\n",
    "    features = features / features_sum\n",
    "\n",
    "    # Standard split indices (based on common Cora implementations)\n",
    "    # We'll use 20 nodes per class for training, 500 for val, 1000 for test\n",
    "    train_mask = np.zeros(N, dtype=bool)\n",
    "    val_mask = np.zeros(N, dtype=bool)\n",
    "    test_mask = np.zeros(N, dtype=bool)\n",
    "\n",
    "    # For reproducibility, we use a class-balanced split\n",
    "    np.random.seed(42)\n",
    "    for i in range(len(unique_labels)):\n",
    "        indices = np.where(labels == i)[0]\n",
    "        np.random.shuffle(indices)\n",
    "        train_mask[indices[:20]] = True\n",
    "        val_mask[indices[20 : 20 + 71]] = True  # 71 * 7 = 497\n",
    "        test_mask[indices[20 + 71 : 20 + 71 + 143]] = True  # 143 * 7 = 1001\n",
    "\n",
    "    return (\n",
    "        jnp.array(features),\n",
    "        jnp.array(adj),\n",
    "        jnp.array(labels),\n",
    "        jnp.array(train_mask),\n",
    "        jnp.array(val_mask),\n",
    "        jnp.array(test_mask),\n",
    "    )\n",
    "\n",
    "\n",
    "def loss_fn(model, x, adj, labels, mask, training):\n",
    "    logits = model(x, adj, training=training)\n",
    "    log_probs = jax.nn.log_softmax(logits)\n",
    "    one_hot = jax.nn.one_hot(labels, num_classes=logits.shape[-1])\n",
    "    # Cross entropy only on masked nodes\n",
    "    loss = -jnp.sum(one_hot * log_probs, axis=-1)\n",
    "    return jnp.sum(loss * mask) / jnp.maximum(jnp.sum(mask), 1)\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, x, adj, labels, mask):\n",
    "    grad_fn = nnx.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(model, x, adj, labels, mask, True)\n",
    "    optimizer.update(model, grads)\n",
    "    return loss\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def eval_step(model, x, adj, labels, mask):\n",
    "    logits = model(x, adj, training=False)\n",
    "    preds = jnp.argmax(logits, axis=-1)\n",
    "    correct = jnp.sum((preds == labels) * mask)\n",
    "    total = jnp.sum(mask)\n",
    "    accuracy = correct / jnp.maximum(total, 1)\n",
    "    loss = loss_fn(model, x, adj, labels, mask, False)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db7dd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Cora data...\n",
      "Downloading https://raw.githubusercontent.com/tkipf/pygcn/master/data/cora/cora.content...\n",
      "Downloading https://raw.githubusercontent.com/tkipf/pygcn/master/data/cora/cora.cites...\n",
      "Data loaded. Features: (2708, 1433), Nodes: 2708, Edges: 13264\n",
      "Starting training...\n",
      "Epoch  10: Loss = 1.8906, Val Loss = 1.9067, Val Acc = 0.7485\n",
      "Epoch  20: Loss = 1.8212, Val Loss = 1.8453, Val Acc = 0.7827\n",
      "Epoch  30: Loss = 1.7223, Val Loss = 1.7531, Val Acc = 0.7827\n",
      "Epoch  40: Loss = 1.4675, Val Loss = 1.6313, Val Acc = 0.7746\n",
      "Epoch  50: Loss = 1.3681, Val Loss = 1.4850, Val Acc = 0.8169\n",
      "Epoch  60: Loss = 1.1953, Val Loss = 1.3345, Val Acc = 0.8028\n",
      "Epoch  70: Loss = 1.1052, Val Loss = 1.1954, Val Acc = 0.8089\n",
      "Epoch  80: Loss = 1.0032, Val Loss = 1.0757, Val Acc = 0.8109\n",
      "Epoch  90: Loss = 0.9063, Val Loss = 0.9835, Val Acc = 0.8089\n",
      "Epoch 100: Loss = 0.8257, Val Loss = 0.9084, Val Acc = 0.8149\n",
      "Epoch 110: Loss = 0.7812, Val Loss = 0.8464, Val Acc = 0.8169\n",
      "Epoch 120: Loss = 0.6745, Val Loss = 0.7981, Val Acc = 0.8149\n",
      "Epoch 130: Loss = 0.6951, Val Loss = 0.7553, Val Acc = 0.8089\n",
      "Epoch 140: Loss = 0.6721, Val Loss = 0.7244, Val Acc = 0.8089\n",
      "Epoch 150: Loss = 0.7320, Val Loss = 0.6954, Val Acc = 0.8229\n",
      "Epoch 160: Loss = 0.5734, Val Loss = 0.6827, Val Acc = 0.8109\n",
      "Epoch 170: Loss = 0.5050, Val Loss = 0.6635, Val Acc = 0.7988\n",
      "Epoch 180: Loss = 0.5997, Val Loss = 0.6547, Val Acc = 0.8089\n",
      "Epoch 190: Loss = 0.7087, Val Loss = 0.6414, Val Acc = 0.8008\n",
      "Epoch 200: Loss = 0.4626, Val Loss = 0.6308, Val Acc = 0.7948\n",
      "\n",
      "Final Results:\n",
      "Test Loss: 0.6163\n",
      "Test Accuracy: 0.8032\n",
      "SUCCESS: Accuracy is above 80%\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Cora data...\")\n",
    "x, adj, labels, train_mask, val_mask, test_mask = load_data()\n",
    "print(f\"Data loaded. Features: {x.shape}, Nodes: {x.shape[0]}, Edges: {jnp.sum(adj > 0)}\")\n",
    "\n",
    "key = jax.random.key(42)\n",
    "model_key, _ = jax.random.split(key)\n",
    "\n",
    "config = GATConfig(\n",
    "    in_features=x.shape[1],\n",
    "    hidden_features=8,\n",
    "    out_features=int(jnp.max(labels) + 1),\n",
    "    num_heads=8,\n",
    "    num_out_heads=1,\n",
    "    dropout_prob=0.6,\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "model = GAT(\n",
    "    in_features=config.in_features,\n",
    "    hidden_features=config.hidden_features,\n",
    "    out_features=config.out_features,\n",
    "    num_heads=config.num_heads,\n",
    "    dropout_rng=model_key,\n",
    "    dropout_prob=config.dropout_prob,\n",
    "    alpha=config.alpha,\n",
    "    num_out_heads=config.num_out_heads,\n",
    ")\n",
    "\n",
    "# Standard Adam optimizer for GAT\n",
    "# Paper uses lr=0.005 and weight_decay=5e-4\n",
    "optimizer = nnx.Optimizer(model, optax.adam(learning_rate=0.005), wrt=nnx.Param)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "best_val_acc = 0\n",
    "for epoch in range(1, 201):\n",
    "    loss = train_step(model, optimizer, x, adj, labels, train_mask)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        val_loss, val_acc = eval_step(model, x, adj, labels, val_mask)\n",
    "        print(f\"Epoch {epoch:3d}: Loss = {loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "test_loss, test_acc = eval_step(model, x, adj, labels, test_mask)\n",
    "print(\"\\nFinal Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "if test_acc >= 0.80:\n",
    "    print(\"SUCCESS: Accuracy is above 80%\")\n",
    "else:\n",
    "    print(\"FAILURE: Accuracy is below 80%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91c18a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
