{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jenriver/bonsai/blob/aistack/bonsai/models/unetr/tests/unetr_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Y5vAPT-pmYF"
   },
   "source": [
    "# Image Segmentation with UNETR\n",
    "\n",
    "This notebook demonstrates training a UNETR model for image segmentation on the Oxford-IIIT Pet Dataset.\n",
    "It uses the UNETR implementation from `bonsai.models.unetr`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmZ4WuADpmYH"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aY0aafhTpmYI"
   },
   "source": [
    "## 1. Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MAzXQgFr-Y1"
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/jax-ml/bonsai\n",
    "!pip install -U -q opencv-python-headless grain albumentations Pillow\n",
    "!pip install -U -q optax orbax-checkpoint\n",
    "!pip install -e -q ./bonsai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51Gj1cohwGJJ"
   },
   "outputs": [],
   "source": [
    "import bonsai\n",
    "\n",
    "print(f\"Bonsai location: {bonsai.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyssiMwWwony"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Any, Callable\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import grain.python as grain\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "from flax import nnx\n",
    "from PIL import Image\n",
    "\n",
    "from bonsai.models.unetr.modeling import UNETR, UNETRConfig\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTMs-pgwpmYJ"
   },
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsj0_VOMpmYJ"
   },
   "outputs": [],
   "source": [
    "# Download Data using standard libraries to avoid shell timeouts\n",
    "import shutil\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "data_root = \"/tmp/data/oxford_pets\"\n",
    "if os.path.exists(data_root):\n",
    "    shutil.rmtree(data_root)\n",
    "os.makedirs(data_root, exist_ok=True)\n",
    "\n",
    "\n",
    "def download_and_extract(url, extract_to):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    filepath = os.path.join(extract_to, filename)\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    urllib.request.urlretrieve(url, filepath)\n",
    "    print(f\"Extracting {filename}...\")\n",
    "    with tarfile.open(filepath, \"r:gz\") as tar:\n",
    "        tar.extractall(path=extract_to)\n",
    "    print(f\"Done {filename}\")\n",
    "\n",
    "\n",
    "download_and_extract(\"https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz\", data_root)\n",
    "download_and_extract(\"https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz\", data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jx3GQhUQpmYJ"
   },
   "outputs": [],
   "source": [
    "root_path = \"/tmp/data/oxford_pets\"\n",
    "img_path = os.path.join(root_path, \"images\")\n",
    "start_mask_path = os.path.join(root_path, \"annotations/trimaps\")\n",
    "\n",
    "all_images = sorted([os.path.join(img_path, x) for x in os.listdir(img_path) if x.endswith(\".jpg\")])\n",
    "print(f\"Total images: {len(all_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPTcmX2epmYJ"
   },
   "outputs": [],
   "source": [
    "class OxfordPetsDataset(grain.MapDataset):\n",
    "    def __init__(self, root_path, img_path, mask_path):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.all_images = sorted([x for x in os.listdir(img_path) if x.endswith(\".jpg\")])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.all_images[idx]\n",
    "        img_uri = os.path.join(self.img_path, img_name)\n",
    "        mask_uri = os.path.join(self.mask_path, img_name.replace(\".jpg\", \".png\"))\n",
    "\n",
    "        image = np.array(Image.open(img_uri).convert(\"RGB\"))\n",
    "\n",
    "        # Some masks might be missing or corrupted, handle gracefully or assume they exist as per tutorial\n",
    "        try:\n",
    "            mask = np.array(Image.open(mask_uri))\n",
    "        except Exception:\n",
    "            # Fallback for missing masks? Or just skip?\n",
    "            # Unet tutorial assumes standard dataset structure.\n",
    "            # Creating dummy mask if missing for robustness in demo\n",
    "            mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "\n",
    "        # Preprocess mask: 1 -> foreground, 2 -> background, 3 -> boundary.\n",
    "        # We want 0, 1, 2 classes.\n",
    "        mask = mask.astype(np.uint8)\n",
    "        mask = mask - 1\n",
    "        mask = np.clip(mask, 0, 2)\n",
    "\n",
    "        return {\"image\": image, \"mask\": mask}\n",
    "\n",
    "\n",
    "val_len = 200\n",
    "train_dataset = OxfordPetsDataset(root_path, img_path, start_mask_path)\n",
    "# Simple split\n",
    "all_indices = np.arange(len(train_dataset))\n",
    "train_indices = all_indices[:-val_len]\n",
    "val_indices = all_indices[-val_len:]\n",
    "\n",
    "\n",
    "class Subset(grain.MapDataset):\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "\n",
    "train_set = Subset(train_dataset, train_indices)\n",
    "val_set = Subset(train_dataset, val_indices)\n",
    "print(f\"Train size: {len(train_set)}, Val size: {len(val_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DYuYK5zpmYJ"
   },
   "source": [
    "## 3. Transformations and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoQeLvHrpmYJ"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "\n",
    "train_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Rotate(limit=20, p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class DataAugs(grain.MapTransform):\n",
    "    def __init__(self, transforms: Callable):\n",
    "        self.albu_transforms = transforms\n",
    "\n",
    "    def map(self, data):\n",
    "        output = self.albu_transforms(**data)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Parameters\n",
    "batch_size = 8\n",
    "seed = 42\n",
    "\n",
    "# Samplers\n",
    "train_sampler = grain.IndexSampler(\n",
    "    len(train_set), shuffle=True, seed=seed, shard_options=grain.NoSharding(), num_epochs=1\n",
    ")\n",
    "\n",
    "val_sampler = grain.IndexSampler(len(val_set), shuffle=False, seed=seed, shard_options=grain.NoSharding(), num_epochs=1)\n",
    "\n",
    "# Loaders\n",
    "train_loader = grain.DataLoader(\n",
    "    data_source=train_set,\n",
    "    sampler=train_sampler,\n",
    "    worker_count=2,\n",
    "    worker_buffer_size=2,\n",
    "    operations=[\n",
    "        DataAugs(train_transforms),\n",
    "        grain.Batch(batch_size, drop_remainder=True),\n",
    "    ],\n",
    ")\n",
    "\n",
    "val_loader = grain.DataLoader(\n",
    "    data_source=val_set,\n",
    "    sampler=val_sampler,\n",
    "    worker_count=2,\n",
    "    worker_buffer_size=2,\n",
    "    operations=[\n",
    "        DataAugs(val_transforms),\n",
    "        grain.Batch(batch_size),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_VDhRSvpmYK"
   },
   "source": [
    "## 4. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgzOmOLqpmYK"
   },
   "outputs": [],
   "source": [
    "config = UNETRConfig(\n",
    "    out_channels=3,\n",
    "    in_channels=3,\n",
    "    img_size=IMG_SIZE,\n",
    "    # Using a smaller model for demo purpose\n",
    "    hidden_size=252,\n",
    "    mlp_dim=512,\n",
    "    num_heads=6,\n",
    "    num_layers=12,\n",
    "    feature_size=16,\n",
    ")\n",
    "\n",
    "model = UNETR(config=config)\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-kA84Z8pmYK"
   },
   "source": [
    "## 5. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHFdd8JSpmYK"
   },
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "steps_per_epoch = len(train_set) // batch_size\n",
    "total_steps = steps_per_epoch * num_epochs\n",
    "learning_rate = 1e-4\n",
    "momentum = 0.9\n",
    "\n",
    "lr_schedule = optax.linear_schedule(init_value=learning_rate, end_value=0.0, transition_steps=total_steps)\n",
    "\n",
    "# optimizer = nnx.Optimizer(model, optax.adam(learning_rate), wrt=nnx.Param)\n",
    "optimizer = nnx.ModelAndOptimizer(model, optax.adam(learning_rate), wrt=nnx.Param)\n",
    "\n",
    "\n",
    "def compute_softmax_jaccard_loss(logits, masks, reduction=\"mean\"):\n",
    "    y_pred = nnx.softmax(logits, axis=-1)\n",
    "    b, c = y_pred.shape[0], y_pred.shape[-1]\n",
    "    y = nnx.one_hot(masks, num_classes=c, axis=-1)\n",
    "\n",
    "    y_pred = y_pred.reshape((b, -1, c))\n",
    "    y = y.reshape((b, -1, c))\n",
    "\n",
    "    intersection = y_pred * y\n",
    "    union = y_pred + y - intersection + 1e-8\n",
    "\n",
    "    intersection = jnp.sum(intersection, axis=1)\n",
    "    union = jnp.sum(union, axis=1)\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        intersection = jnp.mean(intersection)\n",
    "        union = jnp.mean(union)\n",
    "\n",
    "    return 1.0 - intersection / union\n",
    "\n",
    "\n",
    "def compute_loss(model, inputs, masks):\n",
    "    logits = model(inputs)\n",
    "    ce_loss = optax.softmax_cross_entropy_with_integer_labels(logits, masks).mean()\n",
    "    jacc_loss = compute_softmax_jaccard_loss(logits, masks)\n",
    "    return ce_loss + jacc_loss, (ce_loss, jacc_loss)\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, batch):\n",
    "    inputs = batch[\"image\"]\n",
    "    masks = batch[\"mask\"].astype(jnp.int32)\n",
    "\n",
    "    grad_fn = nnx.value_and_grad(compute_loss, has_aux=True)\n",
    "    (loss, (ce, jacc)), grads = grad_fn(model, inputs, masks)\n",
    "    optimizer.update(grads)\n",
    "    return loss, ce, jacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iI8b1YzepmYK"
   },
   "source": [
    "## 6. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdHorKJapmYK"
   },
   "outputs": [],
   "source": [
    "class ConfusionMatrix(nnx.Metric):\n",
    "    def __init__(self, num_classes: int, average: str | None = None):\n",
    "        self.num_classes = num_classes\n",
    "        self.average = average\n",
    "        self.conf_mat = nnx.metrics.MetricState(jnp.zeros((num_classes, num_classes), dtype=jnp.int32))\n",
    "\n",
    "    def reset(self):\n",
    "        self.conf_mat.value = jnp.zeros((self.num_classes, self.num_classes), dtype=jnp.int32)\n",
    "\n",
    "    def update(self, *, logits: jax.Array, labels: jax.Array, **kwargs):\n",
    "        preds = jnp.argmax(logits, axis=-1)\n",
    "        y_true = labels.reshape(-1)\n",
    "        y_pred = preds.reshape(-1)\n",
    "\n",
    "        # Valid mask: filter out indices not in [0, num_classes)\n",
    "        valid_mask = (y_true >= 0) & (y_true < self.num_classes)\n",
    "\n",
    "        # Calculate linear indices for the confusion matrix.\n",
    "        # We map invalid pixels to an extra bin at index `num_classes**2`\n",
    "        # so they don't corrupt the main matrix.\n",
    "        cm_indices = self.num_classes * y_true + y_pred\n",
    "        cm_indices = jnp.where(valid_mask, cm_indices, self.num_classes**2)\n",
    "\n",
    "        # Compute bincount with fixed size (JIT-friendly)\n",
    "        counts = jnp.bincount(cm_indices, minlength=self.num_classes**2 + 1, length=self.num_classes**2 + 1)\n",
    "\n",
    "        # Discard the invalid bin and reshape\n",
    "        batch_conf_mat = counts[:-1].reshape(self.num_classes, self.num_classes).astype(jnp.int32)\n",
    "        self.conf_mat.value += batch_conf_mat\n",
    "\n",
    "    def compute(self) -> jax.Array:\n",
    "        return self.conf_mat.value\n",
    "\n",
    "\n",
    "def compute_iou(cm: jax.Array) -> jax.Array:\n",
    "    sum_over_row = jnp.sum(cm, axis=0)\n",
    "    sum_over_col = jnp.sum(cm, axis=1)\n",
    "    diag = jnp.diag(cm)\n",
    "    denominator = sum_over_row + sum_over_col - diag\n",
    "    return jnp.where(denominator > 0, diag / denominator, 0.0)\n",
    "\n",
    "\n",
    "def compute_mean_iou(cm: jax.Array) -> jax.Array:\n",
    "    return jnp.mean(compute_iou(cm))\n",
    "\n",
    "\n",
    "def compute_accuracy(cm: jax.Array) -> jax.Array:\n",
    "    return jnp.sum(jnp.diag(cm)) / jnp.sum(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWU00mqWpmYK"
   },
   "source": [
    "## 7. Eval & Train Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0P3qRECpmYK"
   },
   "outputs": [],
   "source": [
    "eval_metrics = nnx.MultiMetric(\n",
    "    total_loss=nnx.metrics.Average(\"total_loss\"),\n",
    "    confusion_matrix=ConfusionMatrix(num_classes=3),\n",
    ")\n",
    "\n",
    "eval_metrics_history = {\n",
    "    \"train_total_loss\": [],\n",
    "    \"train_IoU\": [],\n",
    "    \"train_mean_IoU\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"val_total_loss\": [],\n",
    "    \"val_IoU\": [],\n",
    "    \"val_mean_IoU\": [],\n",
    "    \"val_accuracy\": [],\n",
    "}\n",
    "\n",
    "\n",
    "def compute_losses_and_logits(model, images, masks):\n",
    "    logits = model(images)\n",
    "    # Re-using compute_loss logic but returning logits for metrics\n",
    "    ce_loss = optax.softmax_cross_entropy_with_integer_labels(logits, masks).mean()\n",
    "    jacc_loss = compute_softmax_jaccard_loss(logits, masks)\n",
    "    loss = ce_loss + jacc_loss\n",
    "    return loss, (ce_loss, jacc_loss, logits)\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def eval_step(model: nnx.Module, batch, eval_metrics: nnx.MultiMetric):\n",
    "    inputs = batch[\"image\"]\n",
    "    masks = batch[\"mask\"].astype(jnp.int32)\n",
    "\n",
    "    # We iterate over the batch since the model is usually larger for training\n",
    "    # For eval we might fit simpler, but let's stick to batch processing.\n",
    "    loss, (_, _, logits) = compute_losses_and_logits(model, inputs, masks)\n",
    "    eval_metrics.update(total_loss=loss, logits=logits, labels=masks)\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch, model, optimizer, train_loader):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        loss, ce, jacc = train_step(model, optimizer, batch)\n",
    "        if step % 10 == 0:\n",
    "            print(\n",
    "                f\"\\r[train] epoch: {epoch + 1}/{num_epochs}, iteration: {step}/{steps_per_epoch}, \"\n",
    "                f\"total loss: {loss.item():.4f} \",\n",
    "                f\"xentropy loss: {ce.item():.4f} \",\n",
    "                f\"jaccard loss: {jacc.item():.4f} \",\n",
    "                end=\"\",\n",
    "            )\n",
    "    print(\"\\r\", end=\"\")\n",
    "\n",
    "\n",
    "def evaluate_model(epoch, model, val_loader, eval_metrics, eval_metrics_history):\n",
    "    model.eval()\n",
    "    # Evaluating on validation set only for demo speed\n",
    "    # for tag, eval_loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "    for tag, loader in [(\"val\", val_loader)]:\n",
    "        eval_metrics.reset()\n",
    "        for val_batch in loader:\n",
    "            eval_step(model, val_batch, eval_metrics)\n",
    "\n",
    "        for metric, value in eval_metrics.compute().items():\n",
    "            if metric == \"confusion_matrix\":\n",
    "                eval_metrics_history[f\"{tag}_IoU\"].append(compute_iou(value))\n",
    "                eval_metrics_history[f\"{tag}_mean_IoU\"].append(compute_mean_iou(value))\n",
    "                eval_metrics_history[f\"{tag}_accuracy\"].append(compute_accuracy(value))\n",
    "            else:\n",
    "                eval_metrics_history[f\"{tag}_{metric}\"].append(value)\n",
    "\n",
    "        print(\n",
    "            f\"[{tag}] epoch: {epoch + 1}/{num_epochs} \"\n",
    "            f\"\\n - total loss: {eval_metrics_history[f'{tag}_total_loss'][-1]:0.4f} \"\n",
    "            f\"\\n - IoU per class: {eval_metrics_history[f'{tag}_IoU'][-1].tolist()} \"\n",
    "            f\"\\n - Mean IoU: {eval_metrics_history[f'{tag}_mean_IoU'][-1]:0.4f} \"\n",
    "            f\"\\n - Accuracy: {eval_metrics_history[f'{tag}_accuracy'][-1]:0.4f} \"\n",
    "            \"\\n\"\n",
    "        )\n",
    "    return eval_metrics_history[\"val_mean_IoU\"][-1]\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "checkpoint_path = \"/tmp/output-oxford-model/\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    import shutil\n",
    "\n",
    "    shutil.rmtree(checkpoint_path)\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "options = ocp.CheckpointManagerOptions(max_to_keep=2, create=True)\n",
    "mngr = ocp.CheckpointManager(os.path.abspath(checkpoint_path), options=options)\n",
    "\n",
    "\n",
    "def save_model(epoch, model, mngr):\n",
    "    state = nnx.state(model)\n",
    "    # We should convert PRNGKeyArray to the old format for Dropout layers if needed\n",
    "    # But for demo simple save is mostly sufficient unless using dropout heavily\n",
    "    mngr.save(epoch, args=ocp.args.StandardSave(state))\n",
    "    mngr.wait_until_finished()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFrF6VQ9pmYK"
   },
   "source": [
    "## 8. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3uzowAPpmYK",
    "outputId": "16d26434-05e2-4e30-a4f8-6946b84ce422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-632968660.py:29: DeprecationWarning: '.value' access is now deprecated. For Variable[Array] instances use:\n",
      "\n",
      "  variable[...]\n",
      "\n",
      "For other Variable types use:\n",
      "\n",
      "  variable.get_value()\n",
      "\n",
      "  self.conf_mat.value += batch_conf_mat\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "best_val_mean_iou = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_one_epoch(epoch, model, optimizer, train_loader)\n",
    "\n",
    "    # Evaluate every epoch for demo\n",
    "    val_mean_iou = evaluate_model(epoch, model, val_loader, eval_metrics, eval_metrics_history)\n",
    "\n",
    "    if val_mean_iou > best_val_mean_iou:\n",
    "        print(f\"New best Mean IoU: {val_mean_iou:.4f} (was {best_val_mean_iou:.4f})\")\n",
    "        save_model(epoch, model, mngr)\n",
    "        best_val_mean_iou = val_mean_iou\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjpJtklzpmYK"
   },
   "source": [
    "## 9. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-nszhE-pmYK"
   },
   "outputs": [],
   "source": [
    "epochs_list = list(range(len(eval_metrics_history[\"val_total_loss\"])))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_list, eval_metrics_history[\"val_total_loss\"], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss over Epochs\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_list, eval_metrics_history[\"val_mean_IoU\"], label=\"Validation Mean IoU\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean IoU\")\n",
    "plt.legend()\n",
    "plt.title(\"Mean IoU over Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxdwVAs8pmYK"
   },
   "outputs": [],
   "source": [
    "def display_image_mask_pred(img, mask, pred, label=\"\"):\n",
    "    if img.dtype in (np.float32,):\n",
    "        # Denormalize if simplified, or just display as is if range 0-1\n",
    "        # Assuming standard normalization, we might clipped data.\n",
    "        # Just rescaling for display\n",
    "        img = ((img - img.min()) / (img.max() - img.min()) * 255.0).astype(np.uint8)\n",
    "\n",
    "    _, axs = plt.subplots(1, 5, figsize=(15, 10))\n",
    "    axs[0].set_title(f\"Image{label}\")\n",
    "    axs[0].imshow(img)\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].set_title(f\"Mask{label}\")\n",
    "    axs[1].imshow(mask, vmin=0, vmax=2)\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    axs[2].set_title(\"Image + Mask\")\n",
    "    axs[2].imshow(img)\n",
    "    axs[2].imshow(mask, alpha=0.5, vmin=0, vmax=2)\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "    axs[3].set_title(f\"Pred{label}\")\n",
    "    axs[3].imshow(pred, vmin=0, vmax=2)\n",
    "    axs[3].axis(\"off\")\n",
    "\n",
    "    axs[4].set_title(\"Image + Pred\")\n",
    "    axs[4].imshow(img)\n",
    "    axs[4].imshow(pred, alpha=0.5, vmin=0, vmax=2)\n",
    "    axs[4].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model.eval()\n",
    "val_batch = next(iter(val_loader))\n",
    "\n",
    "images, masks = val_batch[\"image\"], val_batch[\"mask\"]\n",
    "preds_logits = model(images)\n",
    "preds = jnp.argmax(preds_logits, axis=-1)\n",
    "\n",
    "for i in range(min(4, images.shape[0])):\n",
    "    display_image_mask_pred(images[i], masks[i], preds[i], label=\" (val)\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
